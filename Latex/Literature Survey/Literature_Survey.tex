\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm,%
bindingoffset=0mm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.75em depth0em\fi}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{float}
\linespread{1.3}
\raggedbottom




%
\font\reali=msbm10 at 12pt
% subsets of real numbers
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
%

\title{Related Literature}
\author{Marco Brianti}
\date{A.Y. 2018/2019}

\begin{document}
	\large{

\maketitle

\section*{Angeletos and La'O (2010) - NBER Macroeconomics Annual}

They introduce heterogeneous information in a Real Business Cycle model. This assumption can have profound implications for the business cycle. They analyze a standard RBC model with no capital augmented with dispersed information frictions. In particular, economic decisions have to be made under heterogeneous information about the aggregate shocks hitting the economy. They summarize their results as follows. (i) Dispersed information induces inertia in the response of macroeconomic outcomes. (ii) Dispersion of information induces technology shocks to explain only a small fraction of the high-frequency variation in the business cycle. (iii) The drivers of the residual variation in the short-run fluctuations is simply the noise in available information, i.e. correlated errors in the agents' expectations of the fundamental shocks. (iv) These noise-driven fluctuations help formalize a certain type of demand shocks. (v) If a social planner takes information dispersion as given then equilibrium is already efficient implying no room for any intervention.

Importantly, what drivers their results is not per se the level of uncertainty about the underlying fundamental but rather the lack of common knowledge about it. Indeed, their effects are consistent with an arbitrary small level of uncertainty about the underlying fundamentals. However, at the same time, the lack of common knowledge alone does not explain the magnitude of our effects. What they need is also a degree of strategic complementary, which means that dispersed information has an effect if agents care on other agents' choices. Their findings hinge on the combination of heterogeneous information with strong strategic complementarity - but they do not hinge on the level of uncertainty about underlying fundamentals. In addition, notice that standard noise-shock literature obtains fluctuations that vanish when uncertainty on fundamentals is small enough.

Fluctuations are not generated by uncertainty regarding future exogenous fundamentals but by uncertainty regarding future choices of other agents that have different information. Specifically, when information is asymmetric agents face additional uncertainty about the level of economic activity beyond the one they face about fundamentals. It is specifically this feature that differentiated dispersed information different from uncertainty in fundamentals. Conversely, strategic complementary is irrelevant for business cycle fluctuations when information is commonly shared. Interestingly, the larger the level of strategic complementarity the less agents focus on fundamental shocks and the more they focus on public signals attempting to coordinate with each other. Thus, it follows that stronger strategic complementarity induces equilibrium to be more anchored to the past aggregate fundamentals, more sensitive to public information and less sensitive to private information.

Another important point that they stress is that the variance of the idiosyncratic signal received by agents is different from the degree of information dispersion. For example, if the variance of the idiosyncratic noise rises, agents might decide to rely less on their private signal focusing more on the public signal converging expectations and thus decreasing information dispersion. 


\section*{Andrade, Crump, Eusepi, and Moench (2016) - JME}

They study a collection of individual forecasts of real output growth, CPI Inflation and the FFR from the Blue Chip Financial Forecasts (BCFF) survey. They use this dataset to establish three novel stylized facts about forecasters' disagreement. (i) Forecasters disagree both about the sort term but also the medium- and long-run prospects of the economy. (ii) The disagreement among forecasters is time varying, even for long-term forecasts. (iii) The shape of the term structure of disagreement differs markedly across variables. In particular, the term structure of disagreement for GDP is upward sloping, for inflation fairly flat and for the policy rate downward sloping. 

Thus, they rationalize those three key empirical facts with a generalized model of informational frictions which extends the Mankiw and Reis (2002) sticky information framework in two crucial dimensions. (i) It allows for a multivariate setup where agents update information about individual variables at different points in time. (ii) Macroeconomic variables are driven by unobserved short-term and long-term components, introducing an additional filtering problem for the agents. Notice that their model assumes that for each variable and in each period a random fraction of agents does not observe that variable realization. As a result, they do not assume that some agents are systematically more informed than others as in other models previously developed. This is an appealing property in light of the widely documented result that it is difficult to beat consensus forecasts of both survey participants and econometric models. The sticky information model captures the costs of processing the information available to produce a forecast update in the spirit of a rational inattention model. Interestingly, in their model disagreement is an increasing function of both noise and uncertainty. 

The successfully calibrate the model to match previous empirical facts. They only struggle to reproduce the unconditional variance of disagreement over time. They also show that model's feature is able to rationalize disagreement on long-term policy rate accordingly with a standard policy rule.

\section*{Basu and Bundick (2017) - Econometrica}

They argue that macroeconomic comovement is a key empirical feature of the economy's response to an uncertainty shock. Using a structural vector regression (VAR), they identify an uncertainty shock in the data as an exogenous increase in the implied volatility of future stock returns. They use a Cholesky decomposition with the VXO ordered first. This ordering assumes that uncertainty shocks can have an immediate impact on output and its components, but non-uncertainty shocks do not affect the implied stock market volatility impact. Empirically, an uncertainty shock causes statistically significant declines in output, consumption, investment, and hours, with a peak response occurring after about one year. 

Under reasonable assumptions, an increase in uncertainty about the future induces precautionary saving and lowers consumption. Similarly, since both consumption and leisure are normal goods, an increase in uncertainty also induces precautionary labor supply. As current technology and the capital stock remain unchanged, the competitive demand for labor remains unchanged as well. Thus, higher uncertainty reduces consumption but raises output, investment, and hours worked. Yet intuition suggests that the reduction in household expenditure resulting from increased uncertainty could lead to a general decline in output and its components. This intuition is typically correct in models where output is demand-determined (at least over some horizon). In these models, the reduction of consumption demand reduces output and labor input which in turn reduces the demand for capital and hence investment. Aggregate demand-determined output is made consistent with household and firm optimization through endogenous movements in markups which in their model is driven by the standard assumption of nominal price rigidity.

To analyze the quantitative impact of uncertainty shocks, they calibrate and solve a representative-agent, dynamic, stochastic general-equilibrium (DSGE) model with capital accumulation and price rigidity. They examine the effects of second-moment shocks to household discount factors, which they interpret as demand uncertainty. When prices adjust slowly, uncertainty shocks can produce contractions in output and all its components. They calibrate the model using a mixed strategy between IRF-matching and unconditional moment matching. Using simulated data from their model, they show that their empirical identification strategy can recover the true macroeconomic effects of higher uncertainty. 


\section*{Jurado, Ludvigson, and Ng (2015) - AER}


At a general level, uncertainty is typically defined as the conditional volatility of a disturbance that is unforecastable from the perspective of economic agents. In genera equilibrium settings, reasonable mechanisms imply a role for time-varying uncertainty. A challenge in empirically examining the behavior of uncertainty, and its relation to macroeconomic activity, is that no objective measure of uncertainty exists. Unfortunately, the conditions under which common proxies already used in the literature are likely to be tightly linked to the typical theoretical notion of uncertainty may be quite special. Their goal is to provide superior econometric estimates of uncertainty that are free as possible from theoretical impositions and specific-variable fluctuations. They emphasize that what matter for economic decision making is not whether particular economic indicators have become more or less variable or disperse per se, but rather whether the economy has become more or less predictable; that is, less or more uncertain. This implies that the proper measurement of uncertainty requires removing the forecastable component before computing conditional volatility.

Keeping into account previous feature (and also that uncertainty should not depend on a single variable but across the economy), they estimate measure of macroeconomic uncertainty using a monthly macro dataset and uses the information in hundreds of macroeconomic and financial indicators. They find significant independent variation in their estimates of uncertainty as compared to commonly used proxies for uncertainty. This is important because it suggests that much of the variation in common uncertainty proxies is not driven by uncertainty. An important finding is that their estimates imply far fewer large uncertainty episodes than what is inferred from all of the commonly used proxies we study. Moreover, their estimate of macroeconomic uncertainty is far more persistent that stock market volatility: 53 months vs 4 months.

Using an 11-variable monthly macro VAR and recursive identification procedure with uncertainty placed last, they find that common macro uncertainty shocks account for up to 29 percent of the forecast error variance in industrial production. This means that their estimates imply rare uncertainty episodes which are large, persistent and strongly related to economic activity. Although they find that increases in uncertainty are associated with large declines in real activities, they admit that their results are silent on whether uncertainty is the cause or effect of such declines.











}
\end{document}

